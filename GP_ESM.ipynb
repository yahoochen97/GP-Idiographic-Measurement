{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading packages\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from gpytorch.mlls import VariationalELBO\n",
    "\n",
    "from scipy.stats import norm\n",
    "torch.manual_seed(8927)\n",
    "np.random.seed(8927)\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from gpytorch.mlls import VariationalELBO\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from utilities.util import OrdinalLMC, OrdinalLikelihood\n",
    "from utilities.util import correlation_matrix_distance, plot_task_kernel, evaluate_gpr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n"
     ]
    }
   ],
   "source": [
    "load_batch_size = 512\n",
    "num_inducing = 1000\n",
    "num_epochs = 10\n",
    "model_type=\"both\"\n",
    "print(\"loading data...\")\n",
    "data = pd.read_csv(\"./data/loopr_data.csv\", index_col=[0])\n",
    "Items_loopr = data.columns.to_list()\n",
    "\n",
    "# rename volat to violat\n",
    "for i in range(1,5):\n",
    "    Items_loopr[Items_loopr.index(\"Volat.{}\".format(i))] = \"Violat.{}\".format(i)\n",
    "\n",
    "# generate item map from original to current using ESM codebook\n",
    "codebook = pd.read_excel(\"./data/ESM_Codebook.xlsx\")\n",
    "item_mapping = dict(zip([x.replace(\" \", \"\") for x in codebook.iloc[:,0].to_list()],\\\n",
    "                        [x.replace(\" \", \"\") for x in codebook.iloc[:,1].to_list()]))\n",
    "reverse_code = codebook.iloc[:,2].to_list()\n",
    "item_description = codebook.iloc[:,3].to_list()\n",
    "\n",
    "data = pd.read_csv(\"./data/GP_ESM_cleaned.csv\")\n",
    "data.columns = [x.replace(\" \", \"\") for x in data.columns]\n",
    "ESM_items = [x.replace(\" \", \"\") for x in codebook.iloc[:,0].to_list() if x.replace(\" \", \"\") in Items_loopr]\n",
    "reverse_code = [reverse_code[i] for i in range(codebook.shape[0]) if codebook.iloc[i,0].replace(\" \", \"\") in Items_loopr]\n",
    "reverse_code = np.array(reverse_code).reshape(-1,1)\n",
    "time_diff = (pd.to_datetime(data.RecordedDate, format='%Y-%m-%d %H:%M:%S')-pd.to_datetime(data.RecordedDate[0])).dt\n",
    "data[\"day\"] = time_diff.days\n",
    "data[\"day\"] += time_diff.seconds/60/60/25\n",
    "\n",
    "n = data.PID.unique().shape[0]\n",
    "m = len(ESM_items)\n",
    "horizon = data.day.max()\n",
    "\n",
    "train_x = torch.zeros((n*m*data.n.max(),3))\n",
    "train_y = torch.zeros((n*m*data.n.max(),))\n",
    "\n",
    "ITER = 0\n",
    "for iter in range(data.shape[0]):\n",
    "    for j in range(m):\n",
    "        train_x[ITER, 0] = data.PID[iter]\n",
    "        train_x[ITER, 1] = j\n",
    "        train_x[ITER, 2] = data.day[iter]\n",
    "        train_y[ITER] = data[item_mapping[ESM_items[j]]][iter]\n",
    "        ITER += 1\n",
    "\n",
    "train_x = train_x[~train_y.isnan()]\n",
    "train_y = train_y[~train_y.isnan()]\n",
    "train_x = train_x[train_y!=0]\n",
    "train_y = train_y[train_y!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 5\n",
    "Q = 5\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=load_batch_size, shuffle=True)\n",
    "\n",
    "# initialize likelihood and model\n",
    "inducing_points = train_x[np.random.choice(train_x.size(0),num_inducing,replace=False),:]\n",
    "likelihood = OrdinalLikelihood(thresholds=torch.tensor([-20.,-2.,-1.,1.,2.,20.]))\n",
    "model = OrdinalLMC(inducing_points,n=n,m=m,C=C,horizon=horizon,\\\n",
    "                   pop_rank=Q, unit_rank=1, model_type=model_type)\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# initialize covariance of pop factors\n",
    "pop_prior = np.load(\"./results/loopr/loopr_pop.npz\")\n",
    "loopr_idx = [Items_loopr.index(x) for x in ESM_items]\n",
    "model.pop_task_covar_module.covar_factor.data = torch.tensor(pop_prior[\"pop_factor\"][loopr_idx])\n",
    "# reverse_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of model parameters: 549.0\n",
      "start training...\n",
      "Epoch 1 Iter 2 - Loss: 3.572\n",
      "Epoch 1 Iter 3 - Loss: 3.431\n",
      "Epoch 1 Iter 4 - Loss: 3.293\n",
      "Epoch 1 Iter 5 - Loss: 3.117\n",
      "Epoch 1 Iter 6 - Loss: 3.067\n",
      "Epoch 1 Iter 7 - Loss: 3.015\n",
      "Epoch 1 Iter 8 - Loss: 2.992\n",
      "Epoch 1 Iter 9 - Loss: 2.926\n",
      "Epoch 1 Iter 10 - Loss: 2.901\n",
      "Epoch 1 Iter 11 - Loss: 2.852\n",
      "Epoch 1 Iter 12 - Loss: 2.847\n",
      "Epoch 1 Iter 13 - Loss: 2.802\n",
      "Epoch 1 Iter 14 - Loss: 2.757\n",
      "Epoch 1 Iter 15 - Loss: 2.737\n",
      "Epoch 1 Iter 16 - Loss: 2.862\n",
      "Epoch 1 - log lik: -23618.695\n",
      "Epoch 2 Iter 2 - Loss: 2.734\n",
      "Epoch 2 Iter 3 - Loss: 2.720\n",
      "Epoch 2 Iter 4 - Loss: 2.748\n",
      "Epoch 2 Iter 5 - Loss: 2.691\n",
      "Epoch 2 Iter 6 - Loss: 2.710\n",
      "Epoch 2 Iter 7 - Loss: 2.726\n",
      "Epoch 2 Iter 8 - Loss: 2.758\n",
      "Epoch 2 Iter 9 - Loss: 2.716\n",
      "Epoch 2 Iter 10 - Loss: 2.790\n",
      "Epoch 2 Iter 11 - Loss: 2.695\n",
      "Epoch 2 Iter 12 - Loss: 2.795\n",
      "Epoch 2 Iter 13 - Loss: 2.719\n",
      "Epoch 2 Iter 14 - Loss: 2.702\n",
      "Epoch 2 Iter 15 - Loss: 2.662\n",
      "Epoch 2 Iter 16 - Loss: 2.583\n",
      "Epoch 2 - log lik: -21012.895\n",
      "Epoch 3 Iter 2 - Loss: 2.724\n",
      "Epoch 3 Iter 3 - Loss: 2.716\n",
      "Epoch 3 Iter 4 - Loss: 2.692\n",
      "Epoch 3 Iter 5 - Loss: 2.720\n",
      "Epoch 3 Iter 6 - Loss: 2.726\n",
      "Epoch 3 Iter 7 - Loss: 2.746\n",
      "Epoch 3 Iter 8 - Loss: 2.721\n",
      "Epoch 3 Iter 9 - Loss: 2.676\n",
      "Epoch 3 Iter 10 - Loss: 2.686\n",
      "Epoch 3 Iter 11 - Loss: 2.659\n",
      "Epoch 3 Iter 12 - Loss: 2.776\n",
      "Epoch 3 Iter 13 - Loss: 2.718\n",
      "Epoch 3 Iter 14 - Loss: 2.673\n",
      "Epoch 3 Iter 15 - Loss: 2.705\n",
      "Epoch 3 Iter 16 - Loss: 2.804\n",
      "Epoch 3 - log lik: -20902.744\n",
      "Epoch 4 Iter 2 - Loss: 2.723\n",
      "Epoch 4 Iter 3 - Loss: 2.680\n",
      "Epoch 4 Iter 4 - Loss: 2.648\n",
      "Epoch 4 Iter 5 - Loss: 2.710\n",
      "Epoch 4 Iter 6 - Loss: 2.712\n",
      "Epoch 4 Iter 7 - Loss: 2.701\n",
      "Epoch 4 Iter 8 - Loss: 2.692\n",
      "Epoch 4 Iter 9 - Loss: 2.742\n",
      "Epoch 4 Iter 10 - Loss: 2.719\n",
      "Epoch 4 Iter 11 - Loss: 2.697\n",
      "Epoch 4 Iter 12 - Loss: 2.703\n",
      "Epoch 4 Iter 13 - Loss: 2.713\n",
      "Epoch 4 Iter 14 - Loss: 2.724\n",
      "Epoch 4 Iter 15 - Loss: 2.731\n",
      "Epoch 4 Iter 16 - Loss: 2.874\n",
      "Epoch 4 - log lik: -20883.022\n",
      "Epoch 5 Iter 2 - Loss: 2.701\n",
      "Epoch 5 Iter 3 - Loss: 2.701\n",
      "Epoch 5 Iter 4 - Loss: 2.720\n",
      "Epoch 5 Iter 5 - Loss: 2.667\n",
      "Epoch 5 Iter 6 - Loss: 2.694\n",
      "Epoch 5 Iter 7 - Loss: 2.740\n",
      "Epoch 5 Iter 8 - Loss: 2.688\n",
      "Epoch 5 Iter 9 - Loss: 2.697\n",
      "Epoch 5 Iter 10 - Loss: 2.728\n",
      "Epoch 5 Iter 11 - Loss: 2.721\n",
      "Epoch 5 Iter 12 - Loss: 2.721\n",
      "Epoch 5 Iter 13 - Loss: 2.768\n",
      "Epoch 5 Iter 14 - Loss: 2.674\n",
      "Epoch 5 Iter 15 - Loss: 2.708\n",
      "Epoch 5 Iter 16 - Loss: 2.815\n",
      "Epoch 5 - log lik: -20881.785\n",
      "Epoch 6 Iter 2 - Loss: 2.717\n",
      "Epoch 6 Iter 3 - Loss: 2.728\n",
      "Epoch 6 Iter 4 - Loss: 2.743\n",
      "Epoch 6 Iter 5 - Loss: 2.681\n",
      "Epoch 6 Iter 6 - Loss: 2.727\n",
      "Epoch 6 Iter 7 - Loss: 2.718\n",
      "Epoch 6 Iter 8 - Loss: 2.709\n",
      "Epoch 6 Iter 9 - Loss: 2.679\n",
      "Epoch 6 Iter 10 - Loss: 2.706\n",
      "Epoch 6 Iter 11 - Loss: 2.710\n",
      "Epoch 6 Iter 12 - Loss: 2.702\n",
      "Epoch 6 Iter 13 - Loss: 2.686\n",
      "Epoch 6 Iter 14 - Loss: 2.702\n",
      "Epoch 6 Iter 15 - Loss: 2.701\n",
      "Epoch 6 Iter 16 - Loss: 2.872\n",
      "Epoch 6 - log lik: -20893.783\n",
      "Epoch 7 Iter 2 - Loss: 2.706\n",
      "Epoch 7 Iter 3 - Loss: 2.705\n",
      "Epoch 7 Iter 4 - Loss: 2.672\n",
      "Epoch 7 Iter 5 - Loss: 2.728\n",
      "Epoch 7 Iter 6 - Loss: 2.718\n",
      "Epoch 7 Iter 7 - Loss: 2.670\n",
      "Epoch 7 Iter 8 - Loss: 2.691\n",
      "Epoch 7 Iter 9 - Loss: 2.666\n",
      "Epoch 7 Iter 10 - Loss: 2.718\n",
      "Epoch 7 Iter 11 - Loss: 2.724\n",
      "Epoch 7 Iter 12 - Loss: 2.708\n",
      "Epoch 7 Iter 13 - Loss: 2.753\n",
      "Epoch 7 Iter 14 - Loss: 2.757\n",
      "Epoch 7 Iter 15 - Loss: 2.688\n",
      "Epoch 7 Iter 16 - Loss: 2.752\n",
      "Epoch 7 - log lik: -20878.231\n",
      "Epoch 8 Iter 2 - Loss: 2.712\n",
      "Epoch 8 Iter 3 - Loss: 2.672\n",
      "Epoch 8 Iter 4 - Loss: 2.704\n",
      "Epoch 8 Iter 5 - Loss: 2.724\n",
      "Epoch 8 Iter 6 - Loss: 2.718\n",
      "Epoch 8 Iter 7 - Loss: 2.693\n",
      "Epoch 8 Iter 8 - Loss: 2.671\n",
      "Epoch 8 Iter 9 - Loss: 2.730\n",
      "Epoch 8 Iter 10 - Loss: 2.708\n",
      "Epoch 8 Iter 11 - Loss: 2.706\n",
      "Epoch 8 Iter 12 - Loss: 2.738\n",
      "Epoch 8 Iter 13 - Loss: 2.689\n",
      "Epoch 8 Iter 14 - Loss: 2.720\n",
      "Epoch 8 Iter 15 - Loss: 2.716\n",
      "Epoch 8 Iter 16 - Loss: 2.706\n",
      "Epoch 8 - log lik: -20869.635\n",
      "Epoch 9 Iter 2 - Loss: 2.720\n",
      "Epoch 9 Iter 3 - Loss: 2.680\n",
      "Epoch 9 Iter 4 - Loss: 2.693\n",
      "Epoch 9 Iter 5 - Loss: 2.726\n",
      "Epoch 9 Iter 6 - Loss: 2.714\n",
      "Epoch 9 Iter 7 - Loss: 2.711\n",
      "Epoch 9 Iter 8 - Loss: 2.725\n",
      "Epoch 9 Iter 9 - Loss: 2.701\n",
      "Epoch 9 Iter 10 - Loss: 2.739\n",
      "Epoch 9 Iter 11 - Loss: 2.733\n",
      "Epoch 9 Iter 12 - Loss: 2.697\n",
      "Epoch 9 Iter 13 - Loss: 2.715\n",
      "Epoch 9 Iter 14 - Loss: 2.694\n",
      "Epoch 9 Iter 15 - Loss: 2.677\n",
      "Epoch 9 Iter 16 - Loss: 2.664\n",
      "Epoch 9 - log lik: -20870.988\n",
      "Epoch 10 Iter 2 - Loss: 2.698\n",
      "Epoch 10 Iter 3 - Loss: 2.705\n",
      "Epoch 10 Iter 4 - Loss: 2.734\n",
      "Epoch 10 Iter 5 - Loss: 2.758\n",
      "Epoch 10 Iter 6 - Loss: 2.683\n",
      "Epoch 10 Iter 7 - Loss: 2.720\n",
      "Epoch 10 Iter 8 - Loss: 2.706\n",
      "Epoch 10 Iter 9 - Loss: 2.725\n",
      "Epoch 10 Iter 10 - Loss: 2.672\n",
      "Epoch 10 Iter 11 - Loss: 2.712\n",
      "Epoch 10 Iter 12 - Loss: 2.726\n",
      "Epoch 10 Iter 13 - Loss: 2.644\n",
      "Epoch 10 Iter 14 - Loss: 2.748\n",
      "Epoch 10 Iter 15 - Loss: 2.699\n",
      "Epoch 10 Iter 16 - Loss: 2.504\n",
      "Epoch 10 - log lik: -20873.870\n"
     ]
    }
   ],
   "source": [
    "# select hyperparameters to learn\n",
    "for i in range(n):\n",
    "    model.t_covar_module[i].lengthscale = data.day.max() // 3 \n",
    "model.fixed_module.raw_lengthscale.requires_grad = False\n",
    "\n",
    "final_params = list(set(model.parameters()) - \\\n",
    "                    {model.fixed_module.raw_lengthscale}) + \\\n",
    "                list(likelihood.parameters())\n",
    "\n",
    "num_params = 0\n",
    "for p in final_params:\n",
    "    if p.requires_grad:\n",
    "        num_param = np.prod(p.size())\n",
    "        if num_param<num_inducing:\n",
    "            num_params += num_param\n",
    "print(\"num of model parameters: {}\".format(num_params))\n",
    "\n",
    "optimizer = torch.optim.Adam(final_params, lr=0.1)\n",
    "\n",
    "# Our loss object. We're using the VariationalELBO\n",
    "mll = VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "\n",
    "# train GPR\n",
    "print(\"start training...\")\n",
    "for i in range(num_epochs):\n",
    "    log_lik = 0\n",
    "    for j, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch)\n",
    "        loss = -mll(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        log_lik += -loss.item()*y_batch.shape[0]\n",
    "        if j % 50:\n",
    "            print('Epoch %d Iter %d - Loss: %.3f' % (i + 1, j+1, loss.item()))\n",
    "    print('Epoch %d - log lik: %.3f' % (i + 1, log_lik))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in-sample evaluatiion...\n",
      "train acc: 0.36186770428015563\n",
      "train ll: -1.38479083313877\n",
      "out-of-sample evaluatiion...\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "train_acc, train_ll = evaluate_gpr(model, likelihood, train_loader)\n",
    "\n",
    "results = {}\n",
    "print(\"in-sample evaluatiion...\")\n",
    "print(\"train acc: {}\".format(train_acc))\n",
    "print(\"train ll: {}\".format(train_ll))\n",
    "\n",
    "results[\"train_acc\"] = train_acc\n",
    "results[\"train_ll\"] = train_ll\n",
    "results[\"log_lik\"] = log_lik\n",
    "results[\"BIC\"] = num_params*np.log(train_x.size(0)) - 2*log_lik \n",
    "\n",
    "task_kernel = model.pop_task_covar_module.covar_matrix.evaluate().detach().numpy()\n",
    "results[\"pop_covariance\"] = task_kernel\n",
    "results[\"pop_factor\"] = model.pop_task_covar_module.covar_factor.data.detach().numpy()\n",
    "\n",
    "unit_covariance = np.zeros((n,m,m))\n",
    "for i in range(n):\n",
    "    task_kernel = np.zeros((m,m))\n",
    "    task_kernel += model.pop_task_covar_module.covar_matrix.evaluate().detach().numpy()\n",
    "    task_kernel += model.unit_task_covar_module[i].covar_matrix.evaluate().detach().numpy()\n",
    "    unit_covariance[i] = task_kernel\n",
    "    results[\"unit_{}_covariance\".format(i)] = task_kernel\n",
    "\n",
    "PATH = \"./results/GP_ESM/\"\n",
    "if not os.path.exists(PATH):\n",
    "    os.makedirs(PATH)\n",
    "np.savez(PATH+\"pop_5.npz\", **results)\n",
    "\n",
    "item_order = sorted(range(len(ESM_items)), key=lambda k: ESM_items[k])\n",
    "plot_task_kernel(task_kernel[item_order,:][:,item_order], np.array(ESM_items)[item_order], \"./results/GP_ESM/pop_5.pdf\", SORT=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
